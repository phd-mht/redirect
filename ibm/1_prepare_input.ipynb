{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19cb5afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import Iterator, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import types as st\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "import settings as s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf9769f9-0403-4c53-9610-d4f3b85fe6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "STRIP_IDS = True\n",
    "CALCULATE_USD_AMOUNT = True\n",
    "\n",
    "TRX_PARTITIONS = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd40b21f-0f55-40dc-93eb-e449cd1cf5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (\n",
    "    sys.version_info.major,\n",
    "    sys.version_info.minor,\n",
    "    sys.version_info.micro,\n",
    ") != (3, 9, 19):\n",
    "    raise EnvironmentError(\n",
    "        \"Only runs efficiently on Python 3.9.19 (Tested on: Conda 24.1.2 | Apple M3 Pro)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b85caee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/12 20:13:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "config = [\n",
    "    (\"spark.driver.memory\", \"8g\"),\n",
    "    (\"spark.worker.memory\", \"8g\"),\n",
    "]\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"testing\")\n",
    "    .config(conf=SparkConf().setAll(config))\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a2d4cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id(value):\n",
    "    return f\"id-{hash(value)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa090168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000000\n",
      "40000000\n",
      "60000000\n",
      "80000000\n",
      "100000000\n",
      "120000000\n",
      "140000000\n",
      "160000000\n",
      "CPU times: user 2min 5s, sys: 14.1 s, total: 2min 19s\n",
      "Wall time: 2min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "try:\n",
    "    os.remove(s.STAGED_DATA_CSV_LOCATION)\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "\n",
    "mapping = {}\n",
    "with open(s.DATA_FILE) as in_file:\n",
    "    cnt = -1\n",
    "    lines = \"\"\n",
    "    for line in in_file:\n",
    "        cnt += 1\n",
    "        if cnt == 0:\n",
    "            continue\n",
    "        line = line.strip()\n",
    "        line_id = get_id(line)\n",
    "        mapping[line_id] = cnt\n",
    "        lines += f\"{cnt},{line}\\n\"\n",
    "        if not (cnt % 2e7):\n",
    "            print(cnt)\n",
    "            with open(s.STAGED_DATA_CSV_LOCATION, \"a\") as out_file:\n",
    "                out_file.write(lines)\n",
    "                lines = \"\"\n",
    "if lines:\n",
    "    lines = lines.strip()\n",
    "    with open(s.STAGED_DATA_CSV_LOCATION, \"a\") as out_file:\n",
    "        out_file.write(lines)\n",
    "        del lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "467df787",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.remove(s.STAGED_PATTERNS_CSV_LOCATION)\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "lines = \"\"\n",
    "with open(s.PATTERNS_FILE) as in_file:\n",
    "    for line in in_file:\n",
    "        line = line.strip()\n",
    "        if line[:4].isnumeric():\n",
    "            line_id = get_id(line)\n",
    "            cnt = mapping[line_id]\n",
    "            lines += f\"{cnt},{line}\\n\"\n",
    "        else:\n",
    "            lines += f\"{line}\\n\"\n",
    "\n",
    "lines = lines.strip()\n",
    "with open(s.STAGED_PATTERNS_CSV_LOCATION, \"a\") as out_file:\n",
    "    out_file.write(lines)\n",
    "    del lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "304986b5-67dd-431d-b77a-8987b4fcd8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "del mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcd1cf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = st.StructType(\n",
    "    [\n",
    "        st.StructField(\"transaction_id\", st.IntegerType(), False),\n",
    "        st.StructField(\"timestamp\", st.TimestampType(), False),\n",
    "        st.StructField(\"source_bank\", st.StringType(), False),\n",
    "        st.StructField(\"source\", st.StringType(), False),\n",
    "        st.StructField(\"target_bank\", st.StringType(), False),\n",
    "        st.StructField(\"target\", st.StringType(), False),\n",
    "        st.StructField(\"received_amount\", st.FloatType(), False),\n",
    "        st.StructField(\"receiving_currency\", st.StringType(), False),\n",
    "        st.StructField(\"sent_amount\", st.FloatType(), False),\n",
    "        st.StructField(\"sending_currency\", st.StringType(), False),\n",
    "        st.StructField(\"format\", st.StringType(), False),\n",
    "        st.StructField(\"is_laundering\", st.IntegerType(), False),\n",
    "    ]\n",
    ")\n",
    "columns = [x.name for x in schema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c2ee4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(s.STAGED_PATTERNS_CSV_LOCATION, \"r\") as fl:\n",
    "    patterns = fl.read()\n",
    "\n",
    "cases = []\n",
    "case_id = 0\n",
    "for pattern in patterns.split(\"\\n\\n\"):\n",
    "    case_id += 1\n",
    "    if not pattern.strip():\n",
    "        continue\n",
    "    pattern = pattern.split(\"\\n\")\n",
    "    name = pattern.pop(0).split(\" - \")[1]\n",
    "    category, sub_category = name, name\n",
    "    if \": \" in name:\n",
    "        category, sub_category = name.split(\": \")\n",
    "    pattern.pop()\n",
    "    case = pd.DataFrame([x.split(\",\") for x in pattern], columns=columns)\n",
    "    case.loc[:, \"id\"] = case_id\n",
    "    case.loc[:, \"type\"] = category.strip().lower()\n",
    "    case.loc[:, \"sub_type\"] = sub_category.strip().lower()\n",
    "    cases.append(case)\n",
    "cases = pd.concat(cases, ignore_index=True)\n",
    "cases = spark.createDataFrame(cases)\n",
    "cases = cases.withColumn(\"timestamp\", sf.to_timestamp(\"timestamp\", s.TIMESTAMP_FORMAT))\n",
    "cases = cases.select(\"transaction_id\", \"id\", \"type\", \"sub_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45180d75-a337-468b-89e3-38f1b82646f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENCY_MAPPING = {\n",
    "    \"Australian Dollar\": \"aud\",\n",
    "    \"Bitcoin\": \"btc\",\n",
    "    \"Brazil Real\": \"brl\",\n",
    "    \"Canadian Dollar\": \"cad\",\n",
    "    \"Euro\": \"eur\",\n",
    "    \"Mexican Peso\": \"mxn\",\n",
    "    \"Ruble\": \"rub\",\n",
    "    \"Rupee\": \"inr\",\n",
    "    \"Saudi Riyal\": \"sar\",\n",
    "    \"Shekel\": \"ils\",\n",
    "    \"Swiss Franc\": \"chf\",\n",
    "    \"UK Pound\": \"gbp\",\n",
    "    \"US Dollar\": \"usd\",\n",
    "    \"Yen\": \"jpy\",\n",
    "    \"Yuan\": \"cny\",\n",
    "}\n",
    "\n",
    "currency_code = sf.udf(lambda x: CURRENCY_MAPPING[x], st.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edb394c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/12 20:17:38 WARN TaskSetManager: Stage 1 contains a task of very large size (1535 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 11:==========================================>             (12 + 4) / 16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 122 ms, sys: 51.1 ms, total: 173 ms\n",
      "Wall time: 5min 17s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "179504480"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data = spark.read.csv(\n",
    "    s.STAGED_DATA_CSV_LOCATION,\n",
    "    header=False,\n",
    "    schema=schema,\n",
    "    timestampFormat=s.TIMESTAMP_FORMAT,\n",
    ")\n",
    "group_by = [\n",
    "    \"timestamp\",\n",
    "    \"source_bank\",\n",
    "    \"source\",\n",
    "    \"target_bank\",\n",
    "    \"target\",\n",
    "    \"receiving_currency\",\n",
    "    \"sending_currency\",\n",
    "    \"format\",\n",
    "]\n",
    "data = data.groupby(group_by).agg(\n",
    "    sf.first(\"transaction_id\").alias(\"transaction_id\"),\n",
    "    sf.collect_set(\"transaction_id\").alias(\"transaction_ids\"),\n",
    "    sf.sum(\"received_amount\").alias(\"received_amount\"),\n",
    "    sf.sum(\"sent_amount\").alias(\"sent_amount\"),\n",
    "    sf.max(\"is_laundering\").alias(\"is_laundering\"),\n",
    ")\n",
    "data = data.withColumn(\n",
    "    \"source_currency\", currency_code(sf.col(\"sending_currency\"))\n",
    ").withColumn(\n",
    "    \"target_currency\",\n",
    "    currency_code(sf.col(\"receiving_currency\")),\n",
    ")\n",
    "data = data.join(cases, on=\"transaction_id\", how=\"left\").repartition(\n",
    "    TRX_PARTITIONS, \"transaction_id\"\n",
    ")\n",
    "data = data.select(\n",
    "    \"transaction_id\",\n",
    "    \"transaction_ids\",\n",
    "    \"timestamp\",\n",
    "    sf.concat(sf.col(\"source\"), sf.lit(\"-\"), sf.col(\"source_currency\")).alias(\"source\"),\n",
    "    sf.concat(sf.col(\"target\"), sf.lit(\"-\"), sf.col(\"target_currency\")).alias(\"target\"),\n",
    "    \"source_bank\",\n",
    "    \"target_bank\",\n",
    "    \"source_currency\",\n",
    "    \"target_currency\",\n",
    "    sf.col(\"sent_amount\").alias(\"source_amount\"),\n",
    "    sf.col(\"received_amount\").alias(\"target_amount\"),\n",
    "    \"format\",\n",
    "    \"is_laundering\",\n",
    ").persist(StorageLevel.DISK_ONLY)\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72d4b7c6-8740-48ac-9314-25ebf3e56862",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "197749"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.select(sf.explode(\"transaction_ids\")).count() - data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0364dec-3ed8-4213-b7a8-4f6f9eba67dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/12 20:21:13 WARN TaskSetManager: Stage 30 contains a task of very large size (1535 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "cases_data = (\n",
    "    cases.join(\n",
    "        data.withColumnRenamed(\"transaction_id\", \"x\")\n",
    "        .drop(*cases.columns)\n",
    "        .select(sf.explode(\"transaction_ids\").alias(\"transaction_id\"), \"*\"),\n",
    "        on=\"transaction_id\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "    .drop(\"is_laundering\", \"transaction_id\", \"transaction_ids\")\n",
    "    .withColumnRenamed(\"x\", \"transaction_id\")\n",
    ")\n",
    "cases_data.toPandas().to_parquet(s.STAGED_CASES_DATA_LOCATION)\n",
    "cases_data = pd.read_parquet(s.STAGED_CASES_DATA_LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9958c88-72c6-46c2-89fa-cea3635b7ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "currency_rates = {\n",
    "    \"jpy\": np.float32(0.009487665410827868),\n",
    "    \"cny\": np.float32(0.14930721887033868),\n",
    "    \"cad\": np.float32(0.7579775434031815),\n",
    "    \"sar\": np.float32(0.2665884611958837),\n",
    "    \"aud\": np.float32(0.7078143121927827),\n",
    "    \"ils\": np.float32(0.29612081311363503),\n",
    "    \"chf\": np.float32(1.0928961554056371),\n",
    "    \"usd\": np.float32(1.0),\n",
    "    \"eur\": np.float32(1.171783425225877),\n",
    "    \"rub\": np.float32(0.012852809604990688),\n",
    "    \"gbp\": np.float32(1.2916554735187644),\n",
    "    \"btc\": np.float32(11879.132698717296),\n",
    "    \"inr\": np.float32(0.013615817231245796),\n",
    "    \"mxn\": np.float32(0.047296753463246695),\n",
    "    \"brl\": np.float32(0.1771008654705292),\n",
    "}\n",
    "\n",
    "@sf.pandas_udf(st.FloatType())\n",
    "def get_usd_amount(iterator: Iterator[Tuple[pd.Series, pd.Series]]) -> Iterator[pd.Series]:\n",
    "    for a, b in iterator:\n",
    "        yield [currency_rates[x] for x in a] * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8483dd03-18e9-4a29-8f9a-b9e751fcf4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if STRIP_IDS:\n",
    "    data = data.withColumn(\"source\", sf.substring(\"source\", 1, 8))\n",
    "    data = data.withColumn(\"target\", sf.substring(\"target\", 1, 8))\n",
    "if CALCULATE_USD_AMOUNT:\n",
    "    data = data.withColumn(\"amount\", get_usd_amount(\"source_currency\", \"source_amount\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12c5cfc5-cc6f-45b0-b717-bb73c52e9df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "data = data.drop(\"transaction_ids\")\n",
    "data.write.parquet(s.STAGED_DATA_LOCATION, mode=\"overwrite\")\n",
    "data = spark.read.parquet(s.STAGED_DATA_LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15609486-de3c-4470-928e-612aa35a08e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/12 20:22:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/12 20:22:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/12 20:22:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/12 20:22:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/12 20:22:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/12 20:22:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/12 20:22:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/12 20:22:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/12 20:22:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/12 20:22:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/12 20:22:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/12 20:22:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/12 20:22:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/12 20:22:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/12 20:22:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/12 20:22:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/12 20:22:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/12 20:22:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/12 20:22:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/12 20:22:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/12 20:22:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/12 20:22:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/12 20:22:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/12 20:22:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/12 20:23:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "assert data.count() == data.select(\"transaction_id\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4276821f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179504480, 137936, 137933, 16467)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count(), cases_data.shape[0], cases_data[\"transaction_id\"].nunique(), cases_data[\"id\"].nunique()\n",
    "# (179504480, 137936, 137933, 16467)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
