{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5102f290-c8e0-4064-86e8-a89a42e1628e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import shutil\n",
    "import sys\n",
    "import uuid\n",
    "from collections import defaultdict\n",
    "from datetime import timedelta\n",
    "from glob import glob\n",
    "from itertools import product\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import settings as s\n",
    "\n",
    "assert s.FILE_SIZE == \"Large\"\n",
    "assert s.HIGH_ILLICIT == True\n",
    "\n",
    "os.environ[\"EXT_DATA_TYPE_FOLDER\"] = s.OUTPUT_POSTFIX.lstrip(\"-\")\n",
    "\n",
    "from common import get_weights\n",
    "from communities_no_reduction import get_communities_spark\n",
    "from evaluation import cw_confusion_matrix, cw_recall, cw_f1\n",
    "from features import generate_features_spark, generate_features_udf_wrapper, SCHEMA_FEAT_UDF, CURRENCY_RATES\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdfc52ce-1f9d-49d2-81f7-366fcdab6f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_script = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3cbd846-2921-4802-b8b8-38d64092cfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (\n",
    "    sys.version_info.major,\n",
    "    sys.version_info.minor,\n",
    "    sys.version_info.micro,\n",
    ") != (3, 11, 8):\n",
    "    raise EnvironmentError(\n",
    "        \"Only runs efficiently on Python 3.11.8 (Tested on: Conda 24.1.2 | Apple M3 Pro)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adb8939c-e2fc-4bfb-ae18-ef1eec4eafc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/27 10:15:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "SPARK_CONF = [\n",
    "    (\"spark.driver.memory\", \"32g\"),\n",
    "    (\"spark.worker.memory\", \"32g\"),\n",
    "    (\"spark.driver.maxResultSize\", \"32g\"),\n",
    "    (\"spark.driver.bindAddress\", \"127.0.0.1\"),\n",
    "    (\"spark.sql.execution.arrow.pyspark.enabled\", \"true\"),\n",
    "    (\"spark.network.timeout\", \"600s\"),\n",
    "    (\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "]\n",
    "\n",
    "shutil.rmtree(\"artifacts\", ignore_errors=True)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"testing\")\n",
    "    .config(conf=SparkConf().setAll(SPARK_CONF))\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "320b1fa7-53c8-4dbb-b25f-795aaca78a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_main = os.path.join(\"features\", os.environ[\"EXT_DATA_TYPE_FOLDER\"], \"no-reduction\")\n",
    "# shutil.rmtree(location_main, ignore_errors=True)\n",
    "\n",
    "location_communities_leiden = f\"{location_main}{os.sep}communities_leiden.parquet\"\n",
    "\n",
    "location_features_leiden = f\"{location_main}{os.sep}features_leiden.parquet\"\n",
    "location_features_ego = f\"{location_main}{os.sep}features_ego.parquet\"\n",
    "location_features_2_hop = f\"{location_main}{os.sep}features_2_hop.parquet\"\n",
    "location_features_2_hop_out = f\"{location_main}{os.sep}features_2_hop_out.parquet\"\n",
    "location_features_2_hop_in = f\"{location_main}{os.sep}features_2_hop_in.parquet\"\n",
    "location_features_2_hop_combined = f\"{location_main}{os.sep}features_2_hop_combined.parquet\"\n",
    "location_features_source = f\"{location_main}{os.sep}features_source.parquet\"\n",
    "location_features_target = f\"{location_main}{os.sep}features_target.parquet\"\n",
    "\n",
    "location_flow_dispense = f\"{location_main}{os.sep}location_flow_dispense.parquet\"\n",
    "location_flow_passthrough = f\"{location_main}{os.sep}location_flow_passthrough.parquet\"\n",
    "location_flow_sink = f\"{location_main}{os.sep}location_flow_sink.parquet\"\n",
    "\n",
    "location_comm_as_source_features = f\"{location_main}{os.sep}comm_as_source_features.parquet\"\n",
    "location_comm_as_target_features = f\"{location_main}{os.sep}comm_as_target_features.parquet\"\n",
    "location_comm_as_passthrough_features = f\"{location_main}{os.sep}comm_as_passthrough_features.parquet\"\n",
    "location_comm_as_passthrough_features_reverse = f\"{location_main}{os.sep}comm_as_passthrough_features_reverse.parquet\"\n",
    "\n",
    "location_features_node_level = f\"{location_main}{os.sep}features_node_level.parquet\"\n",
    "\n",
    "try:\n",
    "    os.makedirs(location_main)\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82546745-6577-4e38-bf44-1a6114c3a398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.2 ms, sys: 1.53 ms, total: 3.73 ms\n",
      "Wall time: 979 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data = spark.read.parquet(s.STAGED_DATA_LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa2ca303-6778-40af-a6f7-7ac71ebedf67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9664 73599\n",
      "CPU times: user 10.5 s, sys: 129 ms, total: 10.7 s\n",
      "Wall time: 10.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "flows = pd.read_parquet(s.STAGED_CASES_DATA_LOCATION)\n",
    "flows.loc[:, \"src\"] = flows[\"source\"].str.slice(0, 8).tolist()\n",
    "flows.loc[:, \"tgt\"] = flows[\"target\"].str.slice(0, 8).tolist()\n",
    "flows_nodes = set(flows[\"src\"].unique()).union(flows[\"tgt\"].unique())\n",
    "\n",
    "valid_flows = []\n",
    "flows_hash = defaultdict(list)\n",
    "for flow_id, flow in flows.groupby(\"id\"):\n",
    "    flow_nodes = set(flow[\"src\"].unique()).union(flow[\"tgt\"].unique())\n",
    "    # Any flow with less than 2 source/target does not qualify as a \"flow\"\n",
    "    if len(flow_nodes) > 2:\n",
    "        flow_graph = ig.Graph.DataFrame(flow.loc[:, [\"src\", \"tgt\"]], use_vids=False, directed=True)\n",
    "        num_components = len(flow_graph.connected_components(\"weak\"))\n",
    "        if num_components == 1:\n",
    "            valid_flows.append(flow_id)\n",
    "            for f_n in flow_nodes:\n",
    "                flows_hash[f_n].append(flow_id)\n",
    "flows = flows.loc[flows[\"id\"].isin(valid_flows), :].reset_index(drop=True)\n",
    "flows_nodes_filtered = set(flows[\"src\"].unique()).union(flows[\"tgt\"].unique())\n",
    "print(len(valid_flows), len(flows_nodes_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bc0ff86-985f-4deb-a665-221ed6c947a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2092797\n"
     ]
    }
   ],
   "source": [
    "nodes_filtered = data.select(\n",
    "    sf.col(\"source\").alias(\"x\")\n",
    ").union(data.select(sf.col(\"target\").alias(\"x\"))).distinct().toPandas()[\"x\"].tolist()\n",
    "\n",
    "assert (len(set(nodes_filtered).intersection(flows_nodes_filtered)) / len(flows_nodes_filtered)) == 1\n",
    "print(len(nodes_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6cc277c-cf84-487e-bcbd-0784a304cc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52.8 s, sys: 1.15 s, total: 53.9 s\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data = data.withColumn(\"is_laundering\", sf.col(\"is_laundering\").cast(\"boolean\"))\n",
    "edges = data.groupby([\"source\", \"target\"]).agg(\n",
    "    sf.sum(\"amount\").alias(\"amount\")\n",
    ").toPandas()\n",
    "weights = get_weights(edges)\n",
    "edges_agg = edges.set_index([\"source\", \"target\"]).join(\n",
    "    weights.set_index([\"source\", \"target\"]), how=\"left\"\n",
    ").reset_index()\n",
    "edges_agg.loc[:, \"amount_weighted\"] = (\n",
    "    edges_agg.loc[:, \"amount\"] * \n",
    "    (edges_agg.loc[:, \"weight\"] / edges_agg.loc[:, \"weight\"].max())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "199fa227-2ea8-4639-a5ce-fe198af819ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Later on, we will reset the variables (to free up memory), while still keeping these intact\n",
    "to_keep = %who_ls\n",
    "to_keep = list(to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26195fe6-5b17-4525-bf16-83c63098e7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "TOP_N = 50\n",
    "NUM_HOPS = 5\n",
    "\n",
    "data_input = spark.createDataFrame(edges_agg)\n",
    "nodes_source = set(edges_agg[\"source\"].unique())\n",
    "nodes_target = set(edges_agg[\"target\"].unique())\n",
    "nodes_passthrough = nodes_source.intersection(nodes_target)\n",
    "\n",
    "%run generate_flow_features.ipynb\n",
    "\n",
    "comm_as_source_features.to_parquet(location_comm_as_source_features)\n",
    "comm_as_target_features.to_parquet(location_comm_as_target_features)\n",
    "comm_as_passthrough_features.to_parquet(location_comm_as_passthrough_features)\n",
    "comm_as_passthrough_features_reverse.to_parquet(location_comm_as_passthrough_features_reverse)\n",
    "\n",
    "del comm_as_source_features\n",
    "del comm_as_target_features\n",
    "del comm_as_passthrough_features\n",
    "del comm_as_passthrough_features_reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "986d0df2-1737-4f95-ad2b-a73c93bfd259",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Constructing Leiden communities\")\n",
    "\n",
    "graph = ig.Graph.DataFrame(edges_agg.loc[:, [\"source\", \"target\", \"amount_weighted\"]], use_vids=False, directed=True)\n",
    "nodes_mapping = {x.index: x[\"name\"] for x in graph.vs()}\n",
    "communities_leiden = la.find_partition(\n",
    "    graph, la.ModularityVertexPartition, n_iterations=100, weights=\"amount_weighted\"\n",
    ")\n",
    "communities_leiden = [[nodes_mapping[_] for _ in x] for x in communities_leiden]\n",
    "communities_leiden = [(str(uuid.uuid4()), set(x)) for x in communities_leiden]\n",
    "sizes_leiden = [len(x[1]) for x in communities_leiden]\n",
    "\n",
    "with open(location_communities_leiden, \"wb\") as fl:\n",
    "    pickle.dump(communities_leiden, fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26e3f6b2-1caa-4e11-b6ef-65d19f8cc156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(location_communities_leiden, \"rb\") as fl:\n",
    "#     communities_leiden = pickle.load(fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0998de24-a007-40f3-9cad-0703940530c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2092797\n",
      "250000 2092797\n",
      "500000 2092797\n",
      "750000 2092797\n",
      "1000000 2092797\n",
      "1250000 2092797\n",
      "1500000 2092797\n",
      "1750000 2092797\n",
      "2000000 2092797\n",
      "CPU times: user 2min 59s, sys: 3.5 s, total: 3min 2s\n",
      "Wall time: 3min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "KEEP_TOP_N = 100\n",
    "\n",
    "data_agg_weights = get_weights(\n",
    "    data.groupby([\"source\", \"target\"])\n",
    "    .agg(\n",
    "        sf.sum(\"amount\").alias(\"amount\")\n",
    "    ).toPandas()\n",
    ")\n",
    "\n",
    "data_agg_weights_rev = data_agg_weights.rename(\n",
    "    columns={\"target\": \"source\", \"source\": \"target\"}\n",
    ").loc[:, [\"source\", \"target\", \"weight\"]]\n",
    "data_agg_weights_ud = pd.concat([data_agg_weights, data_agg_weights_rev], ignore_index=True)\n",
    "data_agg_weights_ud = data_agg_weights_ud.groupby([\"source\", \"target\"]).agg(weight=(\"weight\", \"sum\")).reset_index()\n",
    "\n",
    "data_agg_weights_ud.sort_values(\"weight\", ascending=False, inplace=True)\n",
    "grouped_ud = data_agg_weights_ud.groupby(\"source\").head(KEEP_TOP_N).reset_index(drop=True)\n",
    "grouped_ud = grouped_ud.groupby(\"source\").agg(targets=(\"target\", set))\n",
    "\n",
    "total = grouped_ud.index.nunique()\n",
    "nodes_neighborhoods = {}\n",
    "for index, (source, targets) in enumerate(grouped_ud.iterrows()):\n",
    "    community_candidates = {source}\n",
    "    for target in targets[\"targets\"]:\n",
    "        community_candidates |= (grouped_ud.loc[target, \"targets\"] | {target})\n",
    "    nodes_neighborhoods[source] = set(community_candidates)\n",
    "    if not (index % 250_000):\n",
    "        print(index, total)\n",
    "\n",
    "del data_agg_weights_rev\n",
    "del data_agg_weights_ud\n",
    "del grouped_ud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e37941e-b6c3-478c-bd54-a16572719410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing 2-hop communities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:>                                                       (0 + 12) / 12]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Constructing 2-hop communities\")\n",
    "\n",
    "communities_2_hop = get_communities_spark(\n",
    "    nodes_neighborhoods,\n",
    "    ig.Graph.DataFrame(edges_agg.loc[:, [\"source\", \"target\", \"amount_weighted\"]], use_vids=False, directed=True), \n",
    "    os.cpu_count(), spark, 2, \"all\", 0.01, \"amount_weighted\"\n",
    ")\n",
    "sizes_2_hop = [len(x[1]) for x in communities_2_hop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03955cf-3674-4fa6-bc72-90939be394e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ts_min = data.select(sf.min(\"timestamp\").alias(\"x\")).collect()[0][\"x\"] - timedelta(minutes=1)\n",
    "data_graph_agg = data.groupby([\"source\", \"target\", \"source_bank\", \"target_bank\", \"source_currency\"]).agg(\n",
    "    sf.count(\"source\").alias(\"num_transactions\"),\n",
    "    sf.sum(\"amount\").alias(\"amount\"),\n",
    "    sf.sum(\"source_amount\").alias(\"source_amount\"),\n",
    "    sf.collect_list(sf.array((sf.col(\"timestamp\") - ts_min).cast(\"long\"), sf.col(\"amount\"))).alias(\"timestamps_amounts\"),\n",
    ")\n",
    "data_graph_agg_sdf = data_graph_agg.persist(StorageLevel.DISK_ONLY)\n",
    "print(data_graph_agg_sdf.count())\n",
    "data_graph_agg = data_graph_agg_sdf.toPandas()\n",
    "index = [\"source\", \"target\"]\n",
    "edges_agg.loc[:, index + [\"weight\"]].set_index(index)\n",
    "data_graph_agg = data_graph_agg.set_index(index).join(\n",
    "    edges_agg.loc[:, index + [\"weight\"]].set_index(index), how=\"left\"\n",
    ").reset_index()\n",
    "data_graph_agg.loc[:, \"amount_weighted\"] = (\n",
    "    data_graph_agg.loc[:, \"amount\"] * \n",
    "    (data_graph_agg.loc[:, \"weight\"] / data_graph_agg.loc[:, \"weight\"].max())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea905a1c-23e7-4f88-b340-59867f2b4bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = ig.Graph.DataFrame(data_graph_agg, use_vids=False, directed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74dce0c8-f8e0-446d-9bfe-e456911434f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Leiden communitites features creation\")\n",
    "\n",
    "features_leiden = generate_features_spark(communities_leiden, graph, spark)\n",
    "features_leiden = features_leiden.rename(columns={\"key\": \"key_fake\"})\n",
    "communities_leiden_dict = dict(communities_leiden)\n",
    "features_leiden.loc[:, \"key\"] = features_leiden.loc[:, \"key_fake\"].apply(lambda x: communities_leiden_dict[x])\n",
    "features_leiden = features_leiden.explode(\"key\")\n",
    "del features_leiden[\"key_fake\"]\n",
    "features_leiden.set_index(\"key\").to_parquet(location_features_leiden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66b1390f-e28d-4def-ae4a-f57766bcf83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(\"2-hop communitites features creation\")\n",
    "\n",
    "features_2_hop = generate_features_spark(communities_2_hop, graph, spark)\n",
    "features_2_hop.set_index(\"key\").to_parquet(location_features_2_hop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d51ba7de-7131-43c7-990e-03a97f4318a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "del graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4c4b527-dab6-4dee-9e19-64c9a74dc15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Temporal flows features creation\")\n",
    "\n",
    "edges_totals = data.select(\"source\", \"target\", \"amount\").groupby(\n",
    "    [\"source\", \"target\"]\n",
    ").agg(sf.count(\"amount\").alias(\"amount\")).toPandas()\n",
    "edges_totals = edges_totals.sort_values(\"amount\", ascending=False).reset_index(drop=True)\n",
    "left_edges = spark.createDataFrame(edges_totals.groupby(\"target\").head(TOP_N).loc[:, [\"source\", \"target\"]])\n",
    "right_edges = spark.createDataFrame(edges_totals.groupby(\"source\").head(TOP_N).loc[:, [\"source\", \"target\"]])\n",
    "\n",
    "columns = [\"source\", \"target\", \"timestamp\", \"amount\"]\n",
    "\n",
    "left = left_edges.select(sf.col(\"source\").alias(\"src\"), sf.col(\"target\").alias(\"tgt\")).join(\n",
    "    data.select(*columns),\n",
    "    on=(sf.col(\"src\") == sf.col(\"source\")) & (sf.col(\"tgt\") == sf.col(\"target\")),\n",
    "    how=\"left\"\n",
    ").drop(\"src\", \"tgt\").persist(StorageLevel.DISK_ONLY)\n",
    "select = []\n",
    "for column in left.columns:\n",
    "    select.append(sf.col(column).alias(f\"left_{column}\"))\n",
    "left = left.select(*select)\n",
    "right = right_edges.select(sf.col(\"source\").alias(\"src\"), sf.col(\"target\").alias(\"tgt\")).join(\n",
    "    data.select(*columns),\n",
    "    on=(sf.col(\"src\") == sf.col(\"source\")) & (sf.col(\"tgt\") == sf.col(\"target\")),\n",
    "    how=\"left\"\n",
    ").drop(\"src\", \"tgt\").persist(StorageLevel.DISK_ONLY)\n",
    "\n",
    "print(left.count(), right.count())\n",
    "\n",
    "flows_temporal = left.join(\n",
    "    right,\n",
    "    (left[\"left_target\"] == right[\"source\"]) &\n",
    "    (left[\"left_timestamp\"] <= right[\"timestamp\"]),\n",
    "    how=\"inner\"\n",
    ").groupby([\"left_source\", \"left_target\", \"source\", \"target\"]).agg(\n",
    "    sf.sum(\"left_amount\").alias(\"left_amount\"),\n",
    "    sf.sum(\"amount\").alias(\"amount\"),\n",
    ").drop(\"left_target\").select(\n",
    "    sf.col(\"left_source\").alias(\"dispense\"),\n",
    "    sf.col(\"source\").alias(\"passthrough\"),\n",
    "    sf.col(\"target\").alias(\"sink\"),\n",
    "    sf.least(\"left_amount\", \"amount\").alias(\"amount\"),\n",
    ")\n",
    "\n",
    "aggregate = [\n",
    "    sf.sum(\"amount\").alias(\"amount_sum\"),\n",
    "    sf.mean(\"amount\").alias(\"amount_mean\"),\n",
    "    sf.median(\"amount\").alias(\"amount_median\"),\n",
    "    sf.max(\"amount\").alias(\"amount_max\"),\n",
    "    sf.stddev(\"amount\").alias(\"amount_std\"),\n",
    "    sf.countDistinct(\"dispense\").alias(\"dispense_count\"),\n",
    "    sf.countDistinct(\"passthrough\").alias(\"passthrough_count\"),\n",
    "    sf.countDistinct(\"sink\").alias(\"sink_count\"),\n",
    "]\n",
    "for flow_location, flow_type in [\n",
    "    (location_flow_dispense, \"dispense\"), (location_flow_passthrough, \"passthrough\"), (location_flow_sink, \"sink\")\n",
    "]:\n",
    "    print(flow_type)\n",
    "    flows_temporal_stats = flows_temporal.groupby(flow_type).agg(*aggregate).toPandas()\n",
    "    flows_temporal_cyclic_stats = flows_temporal.where(\n",
    "        (sf.col(\"dispense\") == sf.col(\"sink\"))\n",
    "    ).groupby(flow_type).agg(*aggregate).toPandas()\n",
    "    flows_temporal_stats = flows_temporal_stats.set_index(flow_type).join(\n",
    "        flows_temporal_cyclic_stats.set_index(flow_type),\n",
    "        how=\"left\", rsuffix=\"_cycle\"\n",
    "    )\n",
    "    flows_temporal_stats.index.name = \"key\"\n",
    "    flows_temporal_stats.to_parquet(flow_location)\n",
    "    del flows_temporal_stats\n",
    "    del flows_temporal_cyclic_stats\n",
    "\n",
    "left.unpersist()\n",
    "right.unpersist()\n",
    "\n",
    "del edges_totals\n",
    "del left_edges\n",
    "del right_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6839964f-cb70-4f50-97be-96b0acdd8355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-hop-source features creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/27 10:22:53 WARN TaskSetManager: Stage 20 contains a task of very large size (8990 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 19s, sys: 21.9 s, total: 3min 41s\n",
      "Wall time: 33min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"1-hop-source features creation\")\n",
    "\n",
    "features_source = spark.createDataFrame(data_graph_agg).withColumn(\n",
    "    \"key\", sf.col(\"source\")\n",
    ").repartition(os.cpu_count() * 5, \"key\").groupby(\"key\").applyInPandas(\n",
    "    generate_features_udf_wrapper(False), schema=SCHEMA_FEAT_UDF\n",
    ").toPandas()\n",
    "features_source = pd.DataFrame(features_source[\"features\"].apply(json.loads).tolist())\n",
    "features_source.columns = [f\"{s.G_1HOP_PREFIX}{x}\" if x != \"key\" else x for x in features_source.columns]\n",
    "features_source.set_index(\"key\").to_parquet(location_features_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb81ceb5-dcdd-4c9e-bb47-1429de489490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-hop-target features creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/27 10:57:32 WARN TaskSetManager: Stage 23 contains a task of very large size (8990 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 24s, sys: 24 s, total: 3min 48s\n",
      "Wall time: 25min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"1-hop-target features creation\")\n",
    "\n",
    "features_target = spark.createDataFrame(data_graph_agg).withColumn(\n",
    "    \"key\", sf.col(\"target\")\n",
    ").repartition(os.cpu_count() * 5, \"key\").groupby(\"key\").applyInPandas(\n",
    "    generate_features_udf_wrapper(False), schema=SCHEMA_FEAT_UDF\n",
    ").toPandas()\n",
    "features_target = pd.DataFrame(features_target[\"features\"].apply(json.loads).tolist())\n",
    "features_target.columns = [f\"{s.G_1HOP_PREFIX}{x}\" if x != \"key\" else x for x in features_target.columns]\n",
    "features_target.set_index(\"key\").to_parquet(location_features_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f335ce88-2ddb-4a5e-82ef-1f2295494bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_graph_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5be6ab9-3a2f-4d27-ad4b-3be73d99f71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENABLED_FEATURES = [\n",
    "    (\"leiden\", location_features_leiden),\n",
    "    (\"2_hop\", location_features_2_hop),\n",
    "    (\"as_source\", location_features_source),\n",
    "    (\"as_target\", location_features_target),\n",
    "    (\"comm_as_source_features\", location_comm_as_source_features),\n",
    "    (\"comm_as_target_features\", location_comm_as_target_features),\n",
    "    (\"comm_as_passthrough_features\", location_comm_as_passthrough_features),\n",
    "    (\"comm_as_passthrough_features_reverse\", location_comm_as_passthrough_features_reverse),\n",
    "    (\"flow_dispense\", location_flow_dispense),\n",
    "    (\"flow_passthrough\", location_flow_passthrough),\n",
    "    (\"flow_sink\", location_flow_sink),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74b28c47-b23c-44e2-b53e-a80b07eb971f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: (2092797, 344)\n"
     ]
    }
   ],
   "source": [
    "all_features = pd.DataFrame()\n",
    "all_features.index.name = \"key\"\n",
    "\n",
    "for feature_group, location in ENABLED_FEATURES:\n",
    "    all_features = all_features.join(\n",
    "        pd.read_parquet(location), how=\"outer\", rsuffix=f\"_{feature_group}\"\n",
    "    )\n",
    "\n",
    "all_features.to_parquet(location_features_node_level)\n",
    "print(\"Features:\", all_features.shape)\n",
    "del all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "85f3470e-c691-4aa7-b273-b4b0041f1dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = pd.read_parquet(location_features_node_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "09a76fe3-abb0-44b4-ac8a-b3b60eb55f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 14 constant columns\n"
     ]
    }
   ],
   "source": [
    "constants = []\n",
    "for column in all_features.columns:\n",
    "    if all_features[column].nunique(dropna=True) <= 1:\n",
    "        del all_features[column]\n",
    "        constants.append(column)\n",
    "print(f\"Deleted {len(constants)} constant columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1963559f-3817-41f4-8ea8-2869faf99e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "medians = {}\n",
    "for column in all_features.columns:\n",
    "    medians[column] = np.nanmedian(all_features[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "322714b7-7063-4485-81b1-6d541d25bb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script executed in 1:05:06\n"
     ]
    }
   ],
   "source": [
    "delta = round(time.time() - start_script)\n",
    "print(f\"Script executed in {timedelta(seconds=delta)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db9398b5-ea2c-48a3-bdc7-934b63ac7520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_turnover_score(df_input):\n",
    "    df_input.loc[:, \"turnover_score\"] = df_input[\"turnover\"] / 100_000\n",
    "    df_input.loc[df_input[\"turnover_score\"] > 1, \"turnover_score\"] = 1\n",
    "    return df_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0e31d8d8-675c-41e5-bcd0-16f4b7d8a2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "flows.loc[:, \"amount\"] = flows.apply(\n",
    "    lambda x: x[\"source_amount\"] * CURRENCY_RATES[x[\"source_currency\"]], axis=1\n",
    ")\n",
    "\n",
    "flow_stats = []\n",
    "for key, df in flows.groupby(\"id\"):\n",
    "    left = (\n",
    "        df.loc[:, [\"tgt\", \"amount\"]]\n",
    "        .rename(columns={\"tgt\": \"src\"})\n",
    "        .groupby(\"src\")\n",
    "        .agg({\"amount\": \"sum\"})\n",
    "    )\n",
    "    right = df.groupby(\"src\").agg({\"amount\": \"sum\"})\n",
    "    result = left.join(right, how=\"outer\", lsuffix=\"_left\").fillna(0).reset_index()\n",
    "    result.loc[:, \"delta\"] = result[\"amount_left\"] - result[\"amount\"]\n",
    "    turnover = float(result[result[\"delta\"] > 0][\"delta\"].sum())\n",
    "    turnover_weight = (\n",
    "        result.set_index(\"src\").apply(\n",
    "            lambda x: max([x[\"amount_left\"], x[\"amount\"]]), axis=1\n",
    "        )\n",
    "        / turnover\n",
    "    ).to_dict()\n",
    "    turnover_weight = {\n",
    "        k: v / (sum(turnover_weight.values()) / len(turnover_weight)) for k, v in turnover_weight.items()\n",
    "    }\n",
    "    nodes = sorted(set(df[\"src\"]).union(df[\"tgt\"]))\n",
    "    flow_stats.append(\n",
    "        {\"id\": key, \"nodes\": nodes, \"turnover\": int(np.ceil(turnover)), \"turnover_weight\": turnover_weight}\n",
    "    )\n",
    "flow_stats = pd.DataFrame(flow_stats)\n",
    "flow_stats = add_turnover_score(flow_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6da3dca-7750-4520-9c78-4fa51c674804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model\n",
      "CPU times: user 6min 18s, sys: 31.1 s, total: 6min 49s\n",
      "Wall time: 6min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Training the model\")\n",
    "\n",
    "anomalies = all_features.loc[:, [\"turnover\"]].fillna(0)\n",
    "model = IsolationForest(n_estimators=10_000)\n",
    "anomalies.loc[:, \"anomaly_score\"] = model.fit(\n",
    "    all_features.fillna(medians)\n",
    ").decision_function(all_features.fillna(medians))\n",
    "anomalies = anomalies.sort_values(\"anomaly_score\", ascending=True)\n",
    "anomalies = add_turnover_score(anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e2b48ca4-110c-4865-9cd6-829fe3fca012",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_50_perc = round(anomalies.shape[0] * 0.5)\n",
    "top_50_perc_normal = anomalies.sort_values(\"anomaly_score\", ascending=False).head(count_50_perc)\n",
    "top_50_perc_normal = set(top_50_perc_normal.index)\n",
    "top_50_perc_anom = anomalies.sort_values(\"anomaly_score\", ascending=True).head(count_50_perc)\n",
    "top_50_perc_anom.to_parquet(\"top_50_perc_anom.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32aee6ed-4792-4ca1-aab2-9bf6115c7bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "communities_2_hop_dict = dict(communities_2_hop)\n",
    "communities_filtered = defaultdict(list)\n",
    "for node, comm in communities_2_hop_dict.items():\n",
    "    comm = tuple(sorted(comm - top_50_perc_normal))\n",
    "    communities_filtered[comm].append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa5f70d-aa9a-44ac-a465-26dcbf1f1893",
   "metadata": {},
   "outputs": [],
   "source": [
    "communities_filtered_rev = {}\n",
    "for key, value in communities_filtered.items():\n",
    "    for x in value:\n",
    "        communities_filtered_rev[x] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895b75fc-8faa-48e2-839f-c7f192c5dbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "libra_ml_accounts = 600\n",
    "libra_anomalies_counts = [385, 770, 1925, 3851]\n",
    "libra_anomalies_actuals_ratio = [x / libra_ml_accounts for x in libra_anomalies_counts]\n",
    "ibm_anomalies_counts = [round(flows[\"id\"].nunique() * x) for x in libra_anomalies_actuals_ratio]\n",
    "ibm_anomalies_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abb2533-3c55-44d8-9a76-e29169bcbfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "results = []\n",
    "for anomalies_count in ibm_anomalies_counts:\n",
    "    processed = set()\n",
    "    top_communities = {}\n",
    "    for index, row in anomalies.iterrows():\n",
    "        comm = set(communities_filtered_rev[index]) - processed\n",
    "        if len(comm) > 2:\n",
    "            processed = processed.union(comm)\n",
    "            top_communities[index] = comm\n",
    "        if len(top_communities) >= anomalies_count:\n",
    "            break\n",
    "    communities_shortlisted = {\n",
    "        k: v for k, v in top_communities.items()\n",
    "    }\n",
    "    max_comm_size = max(\n",
    "        [len(x) for x in communities_shortlisted.values()] + [len(x) for x in flow_stats[\"turnover_weight\"]]\n",
    "    ) + 1\n",
    "    assert len(top_communities) == len(communities_shortlisted)\n",
    "    tp, fp, tn, fn = cw_confusion_matrix(flow_stats, communities_shortlisted, max_comm_size, anomalies, flows_hash)\n",
    "    recall, f1 = cw_recall(tp, fn), cw_f1(tp, fp, fn)\n",
    "    print(f\"{recall=} | {max_comm_size=} | {anomalies_count=}\")\n",
    "    anom_comm_sizes = [len(x) for x in communities_shortlisted.values()]\n",
    "    results.append({\n",
    "        \"anomalies_count\": anomalies_count,\n",
    "        \"recall\": recall,\n",
    "        \"max_comm_size\": max_comm_size,\n",
    "        \"mean_comm_size\": np.mean(anom_comm_sizes),\n",
    "        \"median_comm_size\": np.median(anom_comm_sizes),\n",
    "        \"confusion_matrix\": [tp, fp, tn, fn],\n",
    "    })\n",
    "results = pd.DataFrame(results)\n",
    "results.to_parquet(f\".{os.sep}results{os.sep}{s.OUTPUT_POSTFIX[1:]}-no-reduction.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "608e33a8-e555-4ac3-b846-99f82cce9c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_parquet(f\".{os.sep}results{os.sep}{s.OUTPUT_POSTFIX[1:]}-no-reduction.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
