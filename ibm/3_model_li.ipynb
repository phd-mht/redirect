{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5102f290-c8e0-4064-86e8-a89a42e1628e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import shutil\n",
    "import sys\n",
    "import uuid\n",
    "from collections import defaultdict\n",
    "from datetime import timedelta\n",
    "from glob import glob\n",
    "from itertools import product\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import settings as s\n",
    "\n",
    "assert s.FILE_SIZE == \"Large\"\n",
    "assert s.HIGH_ILLICIT == False\n",
    "\n",
    "os.environ[\"EXT_DATA_TYPE_FOLDER\"] = s.OUTPUT_POSTFIX.lstrip(\"-\")\n",
    "\n",
    "from common import get_weights\n",
    "from communities import get_communities_spark\n",
    "from evaluation import cw_confusion_matrix, cw_recall, cw_f1\n",
    "from features import generate_features_spark, generate_features_udf_wrapper, SCHEMA_FEAT_UDF, CURRENCY_RATES\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdfc52ce-1f9d-49d2-81f7-366fcdab6f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_script = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3cbd846-2921-4802-b8b8-38d64092cfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (\n",
    "    sys.version_info.major,\n",
    "    sys.version_info.minor,\n",
    "    sys.version_info.micro,\n",
    ") != (3, 11, 8):\n",
    "    raise EnvironmentError(\n",
    "        \"Only runs efficiently on Python 3.11.8 (Tested on: Conda 24.1.2 | Apple M3 Pro)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adb8939c-e2fc-4bfb-ae18-ef1eec4eafc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/24 20:53:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "SPARK_CONF = [\n",
    "    (\"spark.driver.memory\", \"32g\"),\n",
    "    (\"spark.worker.memory\", \"32g\"),\n",
    "    (\"spark.driver.maxResultSize\", \"32g\"),\n",
    "    (\"spark.driver.bindAddress\", \"127.0.0.1\"),\n",
    "    (\"spark.sql.execution.arrow.pyspark.enabled\", \"true\"),\n",
    "    (\"spark.network.timeout\", \"600s\"),\n",
    "    (\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "]\n",
    "\n",
    "shutil.rmtree(\"artifacts\", ignore_errors=True)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"testing\")\n",
    "    .config(conf=SparkConf().setAll(SPARK_CONF))\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "320b1fa7-53c8-4dbb-b25f-795aaca78a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "FLOWS_FORMAT_SCOPE = [\"ACH\", \"Bitcoin\", \"Wire\"]\n",
    "MAX_DEGREE_PER_ACCOUNT = 100\n",
    "MAX_TRANSACTIONS_PER_ACCOUNT = 1_000\n",
    "\n",
    "location_main = os.path.join(\"features\", os.environ[\"EXT_DATA_TYPE_FOLDER\"])\n",
    "# shutil.rmtree(location_main, ignore_errors=True)\n",
    "\n",
    "location_communities_leiden = f\"{location_main}{os.sep}communities_leiden.parquet\"\n",
    "\n",
    "location_features_leiden = f\"{location_main}{os.sep}features_leiden.parquet\"\n",
    "location_features_ego = f\"{location_main}{os.sep}features_ego.parquet\"\n",
    "location_features_2_hop = f\"{location_main}{os.sep}features_2_hop.parquet\"\n",
    "location_features_2_hop_out = f\"{location_main}{os.sep}features_2_hop_out.parquet\"\n",
    "location_features_2_hop_in = f\"{location_main}{os.sep}features_2_hop_in.parquet\"\n",
    "location_features_2_hop_combined = f\"{location_main}{os.sep}features_2_hop_combined.parquet\"\n",
    "location_features_source = f\"{location_main}{os.sep}features_source.parquet\"\n",
    "location_features_target = f\"{location_main}{os.sep}features_target.parquet\"\n",
    "\n",
    "location_flow_dispense = f\"{location_main}{os.sep}location_flow_dispense.parquet\"\n",
    "location_flow_passthrough = f\"{location_main}{os.sep}location_flow_passthrough.parquet\"\n",
    "location_flow_sink = f\"{location_main}{os.sep}location_flow_sink.parquet\"\n",
    "\n",
    "location_comm_as_source_features = f\"{location_main}{os.sep}comm_as_source_features.parquet\"\n",
    "location_comm_as_target_features = f\"{location_main}{os.sep}comm_as_target_features.parquet\"\n",
    "location_comm_as_passthrough_features = f\"{location_main}{os.sep}comm_as_passthrough_features.parquet\"\n",
    "location_comm_as_passthrough_features_reverse = f\"{location_main}{os.sep}comm_as_passthrough_features_reverse.parquet\"\n",
    "\n",
    "location_features_node_level = f\"{location_main}{os.sep}features_node_level.parquet\"\n",
    "\n",
    "try:\n",
    "    os.makedirs(location_main)\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82546745-6577-4e38-bf44-1a6114c3a398",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/24 20:55:28 WARN DAGScheduler: Broadcasting large task binary with size 6.2 MiB\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26446978\n",
      "CPU times: user 41 s, sys: 12.6 s, total: 53.6 s\n",
      "Wall time: 2min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data = spark.read.parquet(s.STAGED_DATA_LOCATION)\n",
    "\n",
    "#### [START] Seed selection ####\n",
    "\n",
    "data = data.where(sf.col(\"source\") != sf.col(\"target\"))\n",
    "data = data.where(sf.col(\"format\").isin(FLOWS_FORMAT_SCOPE))\n",
    "\n",
    "large_sources = (\n",
    "    data.groupby(\"source\")\n",
    "    .count()\n",
    "    .where(sf.col(\"count\") > MAX_TRANSACTIONS_PER_ACCOUNT)\n",
    "    .select(\"source\")\n",
    "    .toPandas()[\"source\"]\n",
    "    .tolist()\n",
    ")\n",
    "large_targets = (\n",
    "    data.groupby(\"target\")\n",
    "    .count()\n",
    "    .where(sf.col(\"count\") > MAX_TRANSACTIONS_PER_ACCOUNT)\n",
    "    .select(\"target\")\n",
    "    .toPandas()[\"target\"]\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "large_sources = set(large_sources).union(\n",
    "    data.groupby(\"source\")\n",
    "    .agg(sf.countDistinct(\"target\").alias(\"count\"))\n",
    "    .where(sf.col(\"count\") > MAX_DEGREE_PER_ACCOUNT)\n",
    "    .select(\"source\")\n",
    "    .toPandas()[\"source\"]\n",
    "    .tolist()\n",
    ")\n",
    "large_targets = set(large_targets).union(\n",
    "    data.groupby(\"target\")\n",
    "    .agg(sf.countDistinct(\"source\").alias(\"count\"))\n",
    "    .where(sf.col(\"count\") > MAX_DEGREE_PER_ACCOUNT)\n",
    "    .select(\"target\")\n",
    "    .toPandas()[\"target\"]\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "data = data.where(~sf.col(\"source\").isin(large_sources))\n",
    "data = data.where(~sf.col(\"target\").isin(large_targets))\n",
    "\n",
    "graph = ig.Graph.DataFrame(data.select(\"source\", \"target\").toPandas(), use_vids=False, directed=True)\n",
    "nodes_mapping = {x.index: x[\"name\"] for x in graph.vs()}\n",
    "cc = graph.connected_components(\"weak\")\n",
    "small_components = [x for x in cc if len(x) < 3]\n",
    "small_nodes = [nodes_mapping[x] for y in small_components for x in y]\n",
    "\n",
    "data = data.where(~sf.col(\"source\").isin(small_nodes))\n",
    "data = data.where(~sf.col(\"target\").isin(small_nodes))\n",
    "\n",
    "data = data.repartition(os.cpu_count() * 5).persist(StorageLevel.DISK_ONLY)\n",
    "print(data.count())\n",
    "\n",
    "#### [END] Seed selection ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa2ca303-6778-40af-a6f7-7ac71ebedf67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1360 12473\n",
      "CPU times: user 1.57 s, sys: 61.4 ms, total: 1.63 s\n",
      "Wall time: 1.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "flows = pd.read_parquet(s.STAGED_CASES_DATA_LOCATION)\n",
    "flows.loc[:, \"src\"] = flows[\"source\"].str.slice(0, 8).tolist()\n",
    "flows.loc[:, \"tgt\"] = flows[\"target\"].str.slice(0, 8).tolist()\n",
    "flows_nodes = set(flows[\"src\"].unique()).union(flows[\"tgt\"].unique())\n",
    "\n",
    "valid_flows = []\n",
    "flows_hash = defaultdict(list)\n",
    "for flow_id, flow in flows.groupby(\"id\"):\n",
    "    flow_nodes = set(flow[\"src\"].unique()).union(flow[\"tgt\"].unique())\n",
    "    # Any flow with less than 2 source/target does not qualify as a \"flow\"\n",
    "    if len(flow_nodes) > 2:\n",
    "        flow_graph = ig.Graph.DataFrame(flow.loc[:, [\"src\", \"tgt\"]], use_vids=False, directed=True)\n",
    "        num_components = len(flow_graph.connected_components(\"weak\"))\n",
    "        if num_components == 1:\n",
    "            valid_flows.append(flow_id)\n",
    "            for f_n in flow_nodes:\n",
    "                flows_hash[f_n].append(flow_id)\n",
    "flows = flows.loc[flows[\"id\"].isin(valid_flows), :].reset_index(drop=True)\n",
    "flows_nodes_filtered = set(flows[\"src\"].unique()).union(flows[\"tgt\"].unique())\n",
    "print(len(valid_flows), len(flows_nodes_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bc0ff86-985f-4deb-a665-221ed6c947a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1346071\n"
     ]
    }
   ],
   "source": [
    "nodes_filtered = data.select(\n",
    "    sf.col(\"source\").alias(\"x\")\n",
    ").union(data.select(sf.col(\"target\").alias(\"x\"))).distinct().toPandas()[\"x\"].tolist()\n",
    "\n",
    "assert (len(set(nodes_filtered).intersection(flows_nodes_filtered)) / len(flows_nodes_filtered)) == 1\n",
    "print(len(nodes_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6cc277c-cf84-487e-bcbd-0784a304cc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.5 s, sys: 547 ms, total: 21.1 s\n",
      "Wall time: 30 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data = data.withColumn(\"is_laundering\", sf.col(\"is_laundering\").cast(\"boolean\"))\n",
    "edges = data.groupby([\"source\", \"target\"]).agg(\n",
    "    sf.sum(\"amount\").alias(\"amount\")\n",
    ").toPandas()\n",
    "weights = get_weights(edges)\n",
    "edges_agg = edges.set_index([\"source\", \"target\"]).join(\n",
    "    weights.set_index([\"source\", \"target\"]), how=\"left\"\n",
    ").reset_index()\n",
    "edges_agg.loc[:, \"amount_weighted\"] = (\n",
    "    edges_agg.loc[:, \"amount\"] * \n",
    "    (edges_agg.loc[:, \"weight\"] / edges_agg.loc[:, \"weight\"].max())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "199fa227-2ea8-4639-a5ce-fe198af819ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Later on, we will reset the variables (to free up memory), while still keeping these intact\n",
    "to_keep = %who_ls\n",
    "to_keep = list(to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26195fe6-5b17-4525-bf16-83c63098e7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing comm_as_source\n",
      "\n",
      "Processed hop #1 | 3,419,956 | 1,188,542\n",
      "Processed hop #2 | 3,958,383 | 998,337\n",
      "Processed hop #3 | 4,514,218 | 944,260\n",
      "Processed hop #4 | 4,946,552 | 932,800\n",
      "Processed hop #5 | 5,214,606 | 930,260\n",
      "\n",
      "Processing comm_as_target\n",
      "\n",
      "Processed hop #1 | 3,301,246 | 758,923\n",
      "Processed hop #2 | 3,714,122 | 567,766\n",
      "Processed hop #3 | 4,061,439 | 473,050\n",
      "Processed hop #4 | 4,552,930 | 437,574\n",
      "Processed hop #5 | 5,006,711 | 425,609\n",
      "\n",
      "Processing comm_as_passthrough\n",
      "\n",
      "Processed hop #1 | 1,583,527 | 601,394\n",
      "Processed hop #2 | 1,694,361 | 457,529\n",
      "Processed hop #3 | 1,975,855 | 424,371\n",
      "Processed hop #4 | 2,197,564 | 417,108\n",
      "Processed hop #5 | 2,341,535 | 415,442\n",
      "\n",
      "Processing comm_as_passthrough_reverse\n",
      "\n",
      "Processed hop #1 | 3,070,214 | 601,394\n",
      "Processed hop #2 | 3,431,506 | 454,413\n",
      "Processed hop #3 | 3,677,663 | 387,820\n",
      "Processed hop #4 | 4,052,239 | 362,380\n",
      "Processed hop #5 | 4,406,899 | 353,952\n",
      "\n",
      "\n",
      "comm_as_source_features\n",
      "\n",
      "CPU times: user 3min 2s, sys: 9.18 s, total: 3min 12s\n",
      "Wall time: 3min 17s\n",
      "\n",
      "comm_as_target_features\n",
      "\n",
      "CPU times: user 1min 38s, sys: 4.69 s, total: 1min 43s\n",
      "Wall time: 1min 45s\n",
      "\n",
      "comm_as_passthrough_features\n",
      "\n",
      "CPU times: user 1min 15s, sys: 534 ms, total: 1min 16s\n",
      "Wall time: 1min 15s\n",
      "\n",
      "comm_as_passthrough_features_reverse\n",
      "\n",
      "CPU times: user 1min 12s, sys: 524 ms, total: 1min 13s\n",
      "Wall time: 1min 13s\n",
      "\n",
      "\n",
      "CPU times: user 13min 5s, sys: 26.6 s, total: 13min 31s\n",
      "Wall time: 13min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "TOP_N = 50\n",
    "NUM_HOPS = 5\n",
    "\n",
    "data_input = spark.createDataFrame(edges_agg)\n",
    "nodes_source = set(edges_agg[\"source\"].unique())\n",
    "nodes_target = set(edges_agg[\"target\"].unique())\n",
    "nodes_passthrough = nodes_source.intersection(nodes_target)\n",
    "\n",
    "%run generate_flow_features.ipynb\n",
    "\n",
    "comm_as_source_features.to_parquet(location_comm_as_source_features)\n",
    "comm_as_target_features.to_parquet(location_comm_as_target_features)\n",
    "comm_as_passthrough_features.to_parquet(location_comm_as_passthrough_features)\n",
    "comm_as_passthrough_features_reverse.to_parquet(location_comm_as_passthrough_features_reverse)\n",
    "\n",
    "del comm_as_source_features\n",
    "del comm_as_target_features\n",
    "del comm_as_passthrough_features\n",
    "del comm_as_passthrough_features_reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "986d0df2-1737-4f95-ad2b-a73c93bfd259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing Leiden communities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23min 50s, sys: 13.3 s, total: 24min 4s\n",
      "Wall time: 24min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Constructing Leiden communities\")\n",
    "\n",
    "graph = ig.Graph.DataFrame(edges_agg.loc[:, [\"source\", \"target\", \"amount_weighted\"]], use_vids=False, directed=True)\n",
    "nodes_mapping = {x.index: x[\"name\"] for x in graph.vs()}\n",
    "communities_leiden = la.find_partition(\n",
    "    graph, la.ModularityVertexPartition, n_iterations=100, weights=\"amount_weighted\"\n",
    ")\n",
    "communities_leiden = [[nodes_mapping[_] for _ in x] for x in communities_leiden]\n",
    "communities_leiden = [(str(uuid.uuid4()), set(x)) for x in communities_leiden]\n",
    "sizes_leiden = [len(x[1]) for x in communities_leiden]\n",
    "\n",
    "with open(location_communities_leiden, \"wb\") as fl:\n",
    "    pickle.dump(communities_leiden, fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26e3f6b2-1caa-4e11-b6ef-65d19f8cc156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(location_communities_leiden, \"rb\") as fl:\n",
    "#     communities_leiden = pickle.load(fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e37941e-b6c3-478c-bd54-a16572719410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing 2-hop communities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.8 s, sys: 540 ms, total: 12.3 s\n",
      "Wall time: 3min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Constructing 2-hop communities\")\n",
    "\n",
    "communities_2_hop = get_communities_spark(\n",
    "    [(x, [x]) for x in nodes_filtered],\n",
    "    ig.Graph.DataFrame(edges_agg.loc[:, [\"source\", \"target\", \"amount_weighted\"]], use_vids=False, directed=True), \n",
    "    os.cpu_count(), spark, 2, \"all\", 0.01, \"amount_weighted\"\n",
    ")\n",
    "sizes_2_hop = [len(x[1]) for x in communities_2_hop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d03955cf-3674-4fa6-bc72-90939be394e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3421971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.9 s, sys: 1.94 s, total: 27.8 s\n",
      "Wall time: 51.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ts_min = data.select(sf.min(\"timestamp\").alias(\"x\")).collect()[0][\"x\"] - timedelta(minutes=1)\n",
    "data_graph_agg = data.groupby([\"source\", \"target\", \"source_bank\", \"target_bank\", \"source_currency\"]).agg(\n",
    "    sf.count(\"source\").alias(\"num_transactions\"),\n",
    "    sf.sum(\"amount\").alias(\"amount\"),\n",
    "    sf.sum(\"source_amount\").alias(\"source_amount\"),\n",
    "    sf.collect_list(sf.array((sf.col(\"timestamp\") - ts_min).cast(\"long\"), sf.col(\"amount\"))).alias(\"timestamps_amounts\"),\n",
    ")\n",
    "data_graph_agg_sdf = data_graph_agg.persist(StorageLevel.DISK_ONLY)\n",
    "print(data_graph_agg_sdf.count())\n",
    "data_graph_agg = data_graph_agg_sdf.toPandas()\n",
    "index = [\"source\", \"target\"]\n",
    "edges_agg.loc[:, index + [\"weight\"]].set_index(index)\n",
    "data_graph_agg = data_graph_agg.set_index(index).join(\n",
    "    edges_agg.loc[:, index + [\"weight\"]].set_index(index), how=\"left\"\n",
    ").reset_index()\n",
    "data_graph_agg.loc[:, \"amount_weighted\"] = (\n",
    "    data_graph_agg.loc[:, \"amount\"] * \n",
    "    (data_graph_agg.loc[:, \"weight\"] / data_graph_agg.loc[:, \"weight\"].max())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea905a1c-23e7-4f88-b340-59867f2b4bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = ig.Graph.DataFrame(data_graph_agg, use_vids=False, directed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74dce0c8-f8e0-446d-9bfe-e456911434f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leiden communitites features creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 8s, sys: 23.9 s, total: 1min 31s\n",
      "Wall time: 3min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Leiden communitites features creation\")\n",
    "\n",
    "features_leiden = generate_features_spark(communities_leiden, graph, spark)\n",
    "features_leiden = features_leiden.rename(columns={\"key\": \"key_fake\"})\n",
    "communities_leiden_dict = dict(communities_leiden)\n",
    "features_leiden.loc[:, \"key\"] = features_leiden.loc[:, \"key_fake\"].apply(lambda x: communities_leiden_dict[x])\n",
    "features_leiden = features_leiden.explode(\"key\")\n",
    "del features_leiden[\"key_fake\"]\n",
    "features_leiden.set_index(\"key\").to_parquet(location_features_leiden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66b1390f-e28d-4def-ae4a-f57766bcf83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-hop communitites features creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 7s, sys: 21.8 s, total: 12min 29s\n",
      "Wall time: 33min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"2-hop communitites features creation\")\n",
    "\n",
    "features_2_hop = generate_features_spark(communities_2_hop, graph, spark)\n",
    "features_2_hop.set_index(\"key\").to_parquet(location_features_2_hop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d51ba7de-7131-43c7-990e-03a97f4318a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "del graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4c4b527-dab6-4dee-9e19-64c9a74dc15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal flows features creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26323795 26446862\n",
      "dispense\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passthrough\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sink\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.44 s, sys: 1.33 s, total: 7.77 s\n",
      "Wall time: 8min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Temporal flows features creation\")\n",
    "\n",
    "edges_totals = data.select(\"source\", \"target\", \"amount\").groupby(\n",
    "    [\"source\", \"target\"]\n",
    ").agg(sf.count(\"amount\").alias(\"amount\")).toPandas()\n",
    "edges_totals = edges_totals.sort_values(\"amount\", ascending=False).reset_index(drop=True)\n",
    "left_edges = spark.createDataFrame(edges_totals.groupby(\"target\").head(TOP_N).loc[:, [\"source\", \"target\"]])\n",
    "right_edges = spark.createDataFrame(edges_totals.groupby(\"source\").head(TOP_N).loc[:, [\"source\", \"target\"]])\n",
    "\n",
    "columns = [\"source\", \"target\", \"timestamp\", \"amount\"]\n",
    "\n",
    "left = left_edges.select(sf.col(\"source\").alias(\"src\"), sf.col(\"target\").alias(\"tgt\")).join(\n",
    "    data.select(*columns),\n",
    "    on=(sf.col(\"src\") == sf.col(\"source\")) & (sf.col(\"tgt\") == sf.col(\"target\")),\n",
    "    how=\"left\"\n",
    ").drop(\"src\", \"tgt\").persist(StorageLevel.DISK_ONLY)\n",
    "select = []\n",
    "for column in left.columns:\n",
    "    select.append(sf.col(column).alias(f\"left_{column}\"))\n",
    "left = left.select(*select)\n",
    "right = right_edges.select(sf.col(\"source\").alias(\"src\"), sf.col(\"target\").alias(\"tgt\")).join(\n",
    "    data.select(*columns),\n",
    "    on=(sf.col(\"src\") == sf.col(\"source\")) & (sf.col(\"tgt\") == sf.col(\"target\")),\n",
    "    how=\"left\"\n",
    ").drop(\"src\", \"tgt\").persist(StorageLevel.DISK_ONLY)\n",
    "\n",
    "print(left.count(), right.count())\n",
    "\n",
    "flows_temporal = left.join(\n",
    "    right,\n",
    "    (left[\"left_target\"] == right[\"source\"]) &\n",
    "    (left[\"left_timestamp\"] <= right[\"timestamp\"]),\n",
    "    how=\"inner\"\n",
    ").groupby([\"left_source\", \"left_target\", \"source\", \"target\"]).agg(\n",
    "    sf.sum(\"left_amount\").alias(\"left_amount\"),\n",
    "    sf.sum(\"amount\").alias(\"amount\"),\n",
    ").drop(\"left_target\").select(\n",
    "    sf.col(\"left_source\").alias(\"dispense\"),\n",
    "    sf.col(\"source\").alias(\"passthrough\"),\n",
    "    sf.col(\"target\").alias(\"sink\"),\n",
    "    sf.least(\"left_amount\", \"amount\").alias(\"amount\"),\n",
    ")\n",
    "\n",
    "aggregate = [\n",
    "    sf.sum(\"amount\").alias(\"amount_sum\"),\n",
    "    sf.mean(\"amount\").alias(\"amount_mean\"),\n",
    "    sf.median(\"amount\").alias(\"amount_median\"),\n",
    "    sf.max(\"amount\").alias(\"amount_max\"),\n",
    "    sf.stddev(\"amount\").alias(\"amount_std\"),\n",
    "    sf.countDistinct(\"dispense\").alias(\"dispense_count\"),\n",
    "    sf.countDistinct(\"passthrough\").alias(\"passthrough_count\"),\n",
    "    sf.countDistinct(\"sink\").alias(\"sink_count\"),\n",
    "]\n",
    "for flow_location, flow_type in [\n",
    "    (location_flow_dispense, \"dispense\"), (location_flow_passthrough, \"passthrough\"), (location_flow_sink, \"sink\")\n",
    "]:\n",
    "    print(flow_type)\n",
    "    flows_temporal_stats = flows_temporal.groupby(flow_type).agg(*aggregate).toPandas()\n",
    "    flows_temporal_cyclic_stats = flows_temporal.where(\n",
    "        (sf.col(\"dispense\") == sf.col(\"sink\"))\n",
    "    ).groupby(flow_type).agg(*aggregate).toPandas()\n",
    "    flows_temporal_stats = flows_temporal_stats.set_index(flow_type).join(\n",
    "        flows_temporal_cyclic_stats.set_index(flow_type),\n",
    "        how=\"left\", rsuffix=\"_cycle\"\n",
    "    )\n",
    "    flows_temporal_stats.index.name = \"key\"\n",
    "    flows_temporal_stats.to_parquet(flow_location)\n",
    "    del flows_temporal_stats\n",
    "    del flows_temporal_cyclic_stats\n",
    "\n",
    "left.unpersist()\n",
    "right.unpersist()\n",
    "\n",
    "del edges_totals\n",
    "del left_edges\n",
    "del right_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6839964f-cb70-4f50-97be-96b0acdd8355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-hop-source features creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/24 22:25:56 WARN TaskSetManager: Stage 311 contains a task of very large size (2701 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 26s, sys: 6.6 s, total: 1min 33s\n",
      "Wall time: 15min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"1-hop-source features creation\")\n",
    "\n",
    "features_source = spark.createDataFrame(data_graph_agg).withColumn(\n",
    "    \"key\", sf.col(\"source\")\n",
    ").repartition(os.cpu_count() * 5, \"key\").groupby(\"key\").applyInPandas(\n",
    "    generate_features_udf_wrapper(False), schema=SCHEMA_FEAT_UDF\n",
    ").toPandas()\n",
    "features_source = pd.DataFrame(features_source[\"features\"].apply(json.loads).tolist())\n",
    "features_source.columns = [f\"{s.G_1HOP_PREFIX}{x}\" if x != \"key\" else x for x in features_source.columns]\n",
    "features_source.set_index(\"key\").to_parquet(location_features_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb81ceb5-dcdd-4c9e-bb47-1429de489490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-hop-target features creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/24 22:41:42 WARN TaskSetManager: Stage 314 contains a task of very large size (2701 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 21s, sys: 3.85 s, total: 1min 25s\n",
      "Wall time: 10min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"1-hop-target features creation\")\n",
    "\n",
    "features_target = spark.createDataFrame(data_graph_agg).withColumn(\n",
    "    \"key\", sf.col(\"target\")\n",
    ").repartition(os.cpu_count() * 5, \"key\").groupby(\"key\").applyInPandas(\n",
    "    generate_features_udf_wrapper(False), schema=SCHEMA_FEAT_UDF\n",
    ").toPandas()\n",
    "features_target = pd.DataFrame(features_target[\"features\"].apply(json.loads).tolist())\n",
    "features_target.columns = [f\"{s.G_1HOP_PREFIX}{x}\" if x != \"key\" else x for x in features_target.columns]\n",
    "features_target.set_index(\"key\").to_parquet(location_features_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f335ce88-2ddb-4a5e-82ef-1f2295494bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_graph_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5be6ab9-3a2f-4d27-ad4b-3be73d99f71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENABLED_FEATURES = [\n",
    "    (\"leiden\", location_features_leiden),\n",
    "    (\"2_hop\", location_features_2_hop),\n",
    "    (\"as_source\", location_features_source),\n",
    "    (\"as_target\", location_features_target),\n",
    "    (\"comm_as_source_features\", location_comm_as_source_features),\n",
    "    (\"comm_as_target_features\", location_comm_as_target_features),\n",
    "    (\"comm_as_passthrough_features\", location_comm_as_passthrough_features),\n",
    "    (\"comm_as_passthrough_features_reverse\", location_comm_as_passthrough_features_reverse),\n",
    "    (\"flow_dispense\", location_flow_dispense),\n",
    "    (\"flow_passthrough\", location_flow_passthrough),\n",
    "    (\"flow_sink\", location_flow_sink),   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74b28c47-b23c-44e2-b53e-a80b07eb971f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: (1346071, 344)\n"
     ]
    }
   ],
   "source": [
    "all_features = pd.DataFrame()\n",
    "all_features.index.name = \"key\"\n",
    "\n",
    "for feature_group, location in ENABLED_FEATURES:\n",
    "    all_features = all_features.join(\n",
    "        pd.read_parquet(location), how=\"outer\", rsuffix=f\"_{feature_group}\"\n",
    "    )\n",
    "\n",
    "all_features.to_parquet(location_features_node_level)\n",
    "print(\"Features:\", all_features.shape)\n",
    "del all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85f3470e-c691-4aa7-b273-b4b0041f1dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = pd.read_parquet(location_features_node_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09a76fe3-abb0-44b4-ac8a-b3b60eb55f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 18 constant columns\n"
     ]
    }
   ],
   "source": [
    "constants = []\n",
    "for column in all_features.columns:\n",
    "    if all_features[column].nunique(dropna=True) <= 1:\n",
    "        del all_features[column]\n",
    "        constants.append(column)\n",
    "print(f\"Deleted {len(constants)} constant columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1963559f-3817-41f4-8ea8-2869faf99e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "medians = {}\n",
    "for column in all_features.columns:\n",
    "    medians[column] = np.nanmedian(all_features[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "322714b7-7063-4485-81b1-6d541d25bb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script executed in 1:58:55\n"
     ]
    }
   ],
   "source": [
    "delta = round(time.time() - start_script)\n",
    "print(f\"Script executed in {timedelta(seconds=delta)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db9398b5-ea2c-48a3-bdc7-934b63ac7520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_turnover_score(df_input):\n",
    "    df_input.loc[:, \"turnover_score\"] = df_input[\"turnover\"] / 100_000\n",
    "    df_input.loc[df_input[\"turnover_score\"] > 1, \"turnover_score\"] = 1\n",
    "    return df_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e31d8d8-675c-41e5-bcd0-16f4b7d8a2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "flows.loc[:, \"amount\"] = flows.apply(\n",
    "    lambda x: x[\"source_amount\"] * CURRENCY_RATES[x[\"source_currency\"]], axis=1\n",
    ")\n",
    "\n",
    "flow_stats = []\n",
    "for key, df in flows.groupby(\"id\"):\n",
    "    left = (\n",
    "        df.loc[:, [\"tgt\", \"amount\"]]\n",
    "        .rename(columns={\"tgt\": \"src\"})\n",
    "        .groupby(\"src\")\n",
    "        .agg({\"amount\": \"sum\"})\n",
    "    )\n",
    "    right = df.groupby(\"src\").agg({\"amount\": \"sum\"})\n",
    "    result = left.join(right, how=\"outer\", lsuffix=\"_left\").fillna(0).reset_index()\n",
    "    result.loc[:, \"delta\"] = result[\"amount_left\"] - result[\"amount\"]\n",
    "    turnover = float(result[result[\"delta\"] > 0][\"delta\"].sum())\n",
    "    turnover_weight = (\n",
    "        result.set_index(\"src\").apply(\n",
    "            lambda x: max([x[\"amount_left\"], x[\"amount\"]]), axis=1\n",
    "        )\n",
    "        / turnover\n",
    "    ).to_dict()\n",
    "    turnover_weight = {\n",
    "        k: v / (sum(turnover_weight.values()) / len(turnover_weight)) for k, v in turnover_weight.items()\n",
    "    }\n",
    "    nodes = sorted(set(df[\"src\"]).union(df[\"tgt\"]))\n",
    "    flow_stats.append(\n",
    "        {\"id\": key, \"nodes\": nodes, \"turnover\": int(np.ceil(turnover)), \"turnover_weight\": turnover_weight}\n",
    "    )\n",
    "flow_stats = pd.DataFrame(flow_stats)\n",
    "flow_stats = add_turnover_score(flow_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b6da3dca-7750-4520-9c78-4fa51c674804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model\n",
      "CPU times: user 3min 53s, sys: 21.2 s, total: 4min 14s\n",
      "Wall time: 4min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Training the model\")\n",
    "\n",
    "anomalies = all_features.loc[:, [\"turnover\"]].fillna(0)\n",
    "model = IsolationForest(n_estimators=10_000)\n",
    "anomalies.loc[:, \"anomaly_score\"] = model.fit(\n",
    "    all_features.fillna(medians)\n",
    ").decision_function(all_features.fillna(medians))\n",
    "anomalies = anomalies.sort_values(\"anomaly_score\", ascending=True)\n",
    "anomalies = add_turnover_score(anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e2b48ca4-110c-4865-9cd6-829fe3fca012",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_50_perc = round(anomalies.shape[0] * 0.5)\n",
    "top_50_perc_normal = anomalies.sort_values(\"anomaly_score\", ascending=False).head(count_50_perc)\n",
    "top_50_perc_normal = set(top_50_perc_normal.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "32aee6ed-4792-4ca1-aab2-9bf6115c7bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "communities_2_hop_dict = dict(communities_2_hop)\n",
    "communities_filtered = defaultdict(list)\n",
    "for node, comm in communities_2_hop_dict.items():\n",
    "    comm = tuple(sorted(comm - top_50_perc_normal))\n",
    "    communities_filtered[comm].append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dfa5f70d-aa9a-44ac-a465-26dcbf1f1893",
   "metadata": {},
   "outputs": [],
   "source": [
    "communities_filtered_rev = {}\n",
    "for key, value in communities_filtered.items():\n",
    "    for x in value:\n",
    "        communities_filtered_rev[x] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "895b75fc-8faa-48e2-839f-c7f192c5dbf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[873, 1745, 4363, 8729]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "libra_ml_accounts = 600\n",
    "libra_anomalies_counts = [385, 770, 1925, 3851]\n",
    "libra_anomalies_actuals_ratio = [x / libra_ml_accounts for x in libra_anomalies_counts]\n",
    "ibm_anomalies_counts = [round(flows[\"id\"].nunique() * x) for x in libra_anomalies_actuals_ratio]\n",
    "ibm_anomalies_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5abb2533-3c55-44d8-9a76-e29169bcbfde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall=0.0343 | max_comm_size=104 | anomalies_count=873\n",
      "recall=0.0747 | max_comm_size=104 | anomalies_count=1745\n",
      "recall=0.1904 | max_comm_size=104 | anomalies_count=4363\n",
      "recall=0.3506 | max_comm_size=104 | anomalies_count=8729\n",
      "CPU times: user 7.21 s, sys: 421 ms, total: 7.63 s\n",
      "Wall time: 7.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "results = []\n",
    "for anomalies_count in ibm_anomalies_counts:\n",
    "    processed = set()\n",
    "    top_communities = {}\n",
    "    for index, row in anomalies.iterrows():\n",
    "        comm = set(communities_filtered_rev[index]) - processed\n",
    "        if len(comm) > 2:\n",
    "            processed = processed.union(comm)\n",
    "            top_communities[index] = comm\n",
    "        if len(top_communities) >= anomalies_count:\n",
    "            break\n",
    "    communities_shortlisted = {\n",
    "        k: v for k, v in top_communities.items()\n",
    "    }\n",
    "    max_comm_size = max(\n",
    "        [len(x) for x in communities_shortlisted.values()] + [len(x) for x in flow_stats[\"turnover_weight\"]]\n",
    "    ) + 1\n",
    "    assert len(top_communities) == len(communities_shortlisted)\n",
    "    tp, fp, tn, fn = cw_confusion_matrix(flow_stats, communities_shortlisted, max_comm_size, anomalies, flows_hash)\n",
    "    recall, f1 = cw_recall(tp, fn), cw_f1(tp, fp, fn)\n",
    "    print(f\"{recall=} | {max_comm_size=} | {anomalies_count=}\")\n",
    "    anom_comm_sizes = [len(x) for x in communities_shortlisted.values()]\n",
    "    results.append({\n",
    "        \"anomalies_count\": anomalies_count,\n",
    "        \"recall\": recall,\n",
    "        \"max_comm_size\": max_comm_size,\n",
    "        \"mean_comm_size\": np.mean(anom_comm_sizes),\n",
    "        \"median_comm_size\": np.median(anom_comm_sizes),\n",
    "        \"confusion_matrix\": [tp, fp, tn, fn],\n",
    "    })\n",
    "results = pd.DataFrame(results)\n",
    "results.to_parquet(f\".{os.sep}results{os.sep}{s.OUTPUT_POSTFIX[1:]}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f96cb38-2a24-4310-9263-bbb26862b403",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_parquet(f\".{os.sep}results{os.sep}{s.OUTPUT_POSTFIX[1:]}.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
